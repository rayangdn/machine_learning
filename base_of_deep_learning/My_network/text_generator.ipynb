{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loader(raw_dataset, context_length, batch_size):\n",
    "    tokenized = raw_dataset.split()\n",
    "    indices = torch.randint(low=0, high=(len(tokenized)-context_length), size=(batch_size,)).tolist()\n",
    "    X = []\n",
    "    Y = []\n",
    "    for idx in indices:\n",
    "        X.append(tokenized[idx:idx+context_length])\n",
    "        Y.append(tokenized[idx+1:idx+1+context_length])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [['my', 'old', 'friend', 'how'], ['old', 'friend', 'how', 'are'], ['old', 'friend', 'how', 'are'], ['Hello', 'darkness', 'my', 'old']]\n",
      "Y =  [['old', 'friend', 'how', 'are'], ['friend', 'how', 'are', 'you'], ['friend', 'how', 'are', 'you'], ['darkness', 'my', 'old', 'friend']]\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = \"Hello darkness my old friend how are you\"\n",
    "context_length = 4\n",
    "batch_size = 4\n",
    "X, Y = batch_loader(raw_dataset, context_length, batch_size)\n",
    "print(\"X = \", X)\n",
    "print(\"Y = \", Y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int, context_length: int, model_dim: int, num_blocks: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, model_dim)\n",
    "        self.position_embeddings = nn.Embedding(context_length, model_dim)\n",
    "        self.transformer_blocks = nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            self.transformer_blocks.append(self.TransformerBlock(model_dim, num_heads))\n",
    "        self.final_norm = nn.LayerNorm(model_dim)\n",
    "        self.vocab_projection = nn.Linear(model_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        torch.manual_seed(0)\n",
    "        embedded = self.word_embeddings(context)\n",
    "        context_length = context.shape[1]\n",
    "        positions = torch.arange(context_length)\n",
    "        embedded = embedded + self.position_embeddings(positions)\n",
    "\n",
    "        raw_output = self.vocab_projection(self.final_norm(self.transformer_blocks(embedded)))\n",
    "        # raw_output is batch by context_length by vocab_size\n",
    "\n",
    "        probabilities = nn.functional.softmax(raw_output, dim = -1)\n",
    "        return torch.round(probabilities, decimals=4)\n",
    "    \n",
    "    class TransformerBlock(nn.Module):\n",
    "\n",
    "        class MultiHeadedSelfAttention(nn.Module):\n",
    "\n",
    "            class SingleHeadAttention(nn.Module):\n",
    "                def __init__(self, model_dim: int, head_size: int):\n",
    "                    super().__init__()\n",
    "                    torch.manual_seed(0)\n",
    "                    self.key_gen = nn.Linear(model_dim, head_size, bias=False)\n",
    "                    self.query_gen = nn.Linear(model_dim, head_size, bias=False)\n",
    "                    self.value_gen = nn.Linear(model_dim, head_size, bias=False)\n",
    "                \n",
    "                def forward(self, embedded):\n",
    "                    k = self.key_gen(embedded)\n",
    "                    q = self.query_gen(embedded)\n",
    "                    v = self.value_gen(embedded)\n",
    "\n",
    "                    scores = q @ torch.transpose(k, 1, 2) # @ is the same as torch.matmul()\n",
    "                    context_length, attention_dim = k.shape[1], k.shape[2]\n",
    "                    scores = scores / (attention_dim ** 0.5)\n",
    "\n",
    "                    lower_triangular = torch.tril(torch.ones(context_length, context_length))\n",
    "                    mask = lower_triangular == 0\n",
    "                    scores = scores.masked_fill(mask, float('-inf'))\n",
    "                    scores = nn.functional.softmax(scores, dim = 2)\n",
    "\n",
    "                    return scores @ v\n",
    "                \n",
    "            def __init__(self, model_dim: int, num_heads: int):\n",
    "                super().__init__()\n",
    "                torch.manual_seed(0)\n",
    "                self.att_heads = nn.ModuleList()\n",
    "                for i in range(num_heads):\n",
    "                    self.att_heads.append(self.SingleHeadAttention(model_dim, model_dim // num_heads))\n",
    "\n",
    "            def forward(self, embedded):\n",
    "                head_outputs = []\n",
    "                for head in self.att_heads:\n",
    "                    head_outputs.append(head(embedded))\n",
    "                concatenated = torch.cat(head_outputs, dim = 2)\n",
    "                return concatenated\n",
    "        \n",
    "        class VanillaNeuralNetwork(nn.Module):\n",
    "\n",
    "            def __init__(self, model_dim: int):\n",
    "                super().__init__()\n",
    "                torch.manual_seed(0)\n",
    "                self.up_projection = nn.Linear(model_dim, model_dim * 4)\n",
    "                self.relu = nn.ReLU()\n",
    "                self.down_projection = nn.Linear(model_dim * 4, model_dim)\n",
    "                self.dropout = nn.Dropout(0.2) # using p = 0.2\n",
    "            \n",
    "            def forward(self, x):\n",
    "                torch.manual_seed(0)\n",
    "                return self.dropout(self.down_projection(self.relu(self.up_projection(x))))\n",
    "\n",
    "        def __init__(self, model_dim: int, num_heads: int):\n",
    "            super().__init__()\n",
    "            torch.manual_seed(0)\n",
    "            self.attention = self.MultiHeadedSelfAttention(model_dim, num_heads)\n",
    "            self.linear_network = self.VanillaNeuralNetwork(model_dim)\n",
    "            self.first_norm = nn.LayerNorm(model_dim)\n",
    "            self.second_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "        def forward(self, embedded):\n",
    "            torch.manual_seed(0)\n",
    "            embedded = embedded + self.attention(self.first_norm(embedded)) # skip connection\n",
    "            embedded = embedded + self.linear_network(self.second_norm(embedded)) # another skip connection\n",
    "            return embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, new_chars, context, context_length, int_to_char):\n",
    "    generator = torch.manual_seed(0)\n",
    "    initial_state = generator.get_state()\n",
    "    res = []\n",
    "    for i in range(new_chars):\n",
    "        if len(context.T) > context_length:\n",
    "            context = context[:, -context_length:]\n",
    "        prediction = model(context) # B, T, Vocab_size\n",
    "        last_time_step = prediction[:, -1, :] # B, Vocab_size\n",
    "\n",
    "        probabilities = nn.functional.softmax(last_time_step, dim=-1)\n",
    "        next_char = torch.multinomial(probabilities, 1, generator=generator)\n",
    "        generator.set_state(initial_state)\n",
    "        context = torch.cat((context, next_char), dim=-1)\n",
    "        res.append(int_to_char[next_char.item()])\n",
    "        return ''.join(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(104, 128 , 252, 6, 6)\n",
    "new_chars = 1\n",
    "context = torch.zeros(1, 1, dtype=int)\n",
    "context_length = 128\n",
    "int_to_char={0: '\\n', 1: ' ', 2: '!', 3: '\"', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: '+', 12: ',', 13: '-', 14: '.', 15: '/', 16: '0', 17: '1', 18: '2', 19: '3', 20: '4', 21: '5', 22: '6', 23: '7', 24: '8', 25: '9', 26: ':', 27: ';', 28: '?', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '{', 85: '|', 86: '}', 87: 'à', 88: 'á', 89: 'è', 90: 'é', 91: 'ë', 92: 'ñ', 93: 'ó', 94: 'ú', 95: '\\u2005', 96: '–', 97: '—', 98: '‘', 99: '’', 100: '“', 101: '”', 102: '…', 103: '\\u205f'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, new_chars, context, context_length, int_to_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastIAenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
